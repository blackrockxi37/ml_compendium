{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#необходимые импорты (sklearn, numpy, pandas, загрузка датасета load_diabetes)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Итоги всего 2-го модуля.} \\\\\n",
    "\\text{Имеем матрицу признаков X и вектор таргета Y:} \\\\\n",
    "\\left(\\begin{matrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33}\n",
    "\\end{matrix}\\right)\n",
    "\\left(\\begin{matrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{matrix}\\right)\n",
    "= \n",
    "\\left(\\begin{matrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{matrix}\\right) \\\\\n",
    "\\text{Добавляя вектор весов } \\beta_i \\text{ мы можем создать определённую } \\textit{модель машинного обучения.} \\\\\n",
    "\\text{Такая модель называется } \\textit{линейной.} \\\\\n",
    "a(x) = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + ... + \\beta_n x_n \\\\\n",
    "\\text{Так же мы имеем определенные метрики или } \\textit{функционалы качества.} \\\\\n",
    "\\text{Например MSE - mean squared error (средне-квадратичная ошибка).} \\\\\n",
    "||y - a(x)||^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y - x_i \\beta_i)^2 = \\frac{1}{n}((y_1 - x_1 \\beta_1)^2 + (y_2 - x_2 \\beta_2)^2 + ... + (y_n - x_n \\beta_n)^2) \\\\\n",
    "\\text{Или RMSE - root mean squared error:} \\\\\n",
    "\\sqrt{MSE} = \\sqrt{||y - a(x)||^2} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y - x_i \\beta_i)^2} \\\\\n",
    "\\text{Так как нам необхожимо, чтобы данная (или любая другая) метрика была минимальной, мы можем найти коэффициенты (веса), }\\\\\n",
    "\\textit{в экстремуме этой функций.} \\text{(минимуме)} \\\\\n",
    "||y - a(x)||^2 \\rightarrow min \\\\\n",
    "\\text{В таком случае наша задача сводится к поиску экстремальных точек, то есть, к } \\textit{производной.} \\\\\n",
    "\\text{Линейная регрессия находит такую точку экстремума (в многомерном пространстве признаков) следующим образом:} \\\\\n",
    "Y = X \\cdot \\beta_M + \\varepsilon \\\\\n",
    "\\beta_M = (X^T \\cdot X) ^{-1} \\cdot X^T \\cdot Y, \\\\\n",
    "\\beta_M - вектор \\ весов  \\ модели, \\ \\varepsilon \\text{ - вектор случайных ошибок.} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Реализация ЛИНЕЙНОЙ РЕГРЕССИИ из коробки\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X, y)\n",
    "preds1 = model1.predict(X_test)\n",
    "no_cross_rmse = np.sqrt(mse(y_test, preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ручная реализация ЛИНЕЙНОЙ РЕГРЕССИИ\n",
    "class LR:\n",
    "    def __init__(self, fit_intercept = True):\n",
    "        self.intercept = fit_intercept\n",
    "        self.error = False\n",
    "    def fit(self, X, y):\n",
    "        if self.intercept:\n",
    "            X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "        X1 = np.dot(X.T, X)\n",
    "        if np.linalg.det(X1) == 0:\n",
    "            print('Нет обратной матрицы, |X| = 0')\n",
    "            self.error = 1\n",
    "            return\n",
    "        X2 = np.linalg.inv(X1)\n",
    "        X3 = np.dot(X2, X.T)    \n",
    "        self.B = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.intercept = self.B[-1]\n",
    "        self.B = self.B[:-1]\n",
    "    def predict(self, X):\n",
    "        if self.error == 1:\n",
    "            print('Признаки мультиколлинеарны.')\n",
    "            return self.error\n",
    "        if self.intercept:\n",
    "            X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "            self.Beta = np.append(self.B, self.intercept)\n",
    "        else:\n",
    "            self.Beta = self.B\n",
    "        return np.dot(X, self.Beta)\n",
    "\n",
    "model2 = LR()\n",
    "model2.fit(X, y)\n",
    "preds2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn RMSE:  53.85344583676592\n",
      "наша RSME:     53.853445836765964\n",
      "sklearn коэффициенты: [  37.90402135 -241.96436231  542.42875852  347.70384391 -931.48884588] ...\n",
      "наши коэффициенты:    [ 151.34560454   37.90402135 -241.96436231  542.42875852  347.70384391] ...\n",
      "sklearn epsilon: 151.34560453985995\n",
      "наша epsilon:    48.67065743196514\n"
     ]
    }
   ],
   "source": [
    "#сравнение метрик\n",
    "print('sklearn RMSE: ', (mse(y_test, preds1)) ** 0.5)\n",
    "print('наша RSME:    ', (mse(y_test, preds2)) ** 0.5)\n",
    "print('sklearn коэффициенты:', model1.coef_[0:5], '...')\n",
    "print('наши коэффициенты:   ', model2.B[0:5], '...')\n",
    "print('sklearn epsilon:', model1.intercept_)\n",
    "print('наша epsilon:   ', model2.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Второй способ искать минимум у функционала качества - градиентный спуск.} \\\\\n",
    "\\textit{Градиент - } \\text{это вектор, который показывает направление наискорейшего возрастания функции в конкретной точке.} \\\\\n",
    "\\text{Определяется как } \\textit{вектор производных функции} \\text{ по каждой из переменных.} \\\\\n",
    "\\nabla f = \\left(\\frac{df}{dx_1},\\frac{df}{dx_2},\\frac{df}{dx_3},...\\right) \\\\\n",
    "\\text{То есть, если функция - 3-хмерный ланшафт, то градиент в точке будет указывать на крутейший подъем.} \\\\\n",
    "\\text{Это свойство градиента мы можем использовать для нахождения} \\textit{ экстремума функционала качества.} \\\\\n",
    "\\text{Идея градиентного спуска состоит в том, чтобы \"прыгнуть в какую-то точку}X_{нач} \\text{ на многомерном пространстве признаков, } \\\\\n",
    "\\text{а потом начать итеративно двигаться в } \\textit{сторону, противоположную градиенту.} \\\\\n",
    "X_{1} = X_{нач} - \\lambda \\cdot \\nabla f(X_{нач}) \\\\\n",
    "X_{2} = X_{1} - \\lambda \\cdot \\nabla f(X_{1}) \\\\\n",
    "... \\\\\n",
    "X_{конечная} = X_{n-1} - \\lambda \\cdot \\nabla f(X_{n-1}) \\\\\n",
    "\\lambda - ускоритель \\ шага, \\ \\nabla f(X_n) \\  - \\ градиент \\  \\\\\n",
    "\\text{При этом, с каждым шагом } \\lambda \\cdot \\nabla f(X_n) \\text{ значение градиента будет всё сильнее и сильнее стремиться к нулю.} \\\\\n",
    "\\text{Поэтому нам необходимо задать какую-то Эпсилон - окресность } \\varepsilon \\rightarrow 0 \\ ( \\varepsilon \\sim 0\n",
    ") \\text{ и совершать итерации пока шаг не будет меньше } \\varepsilon. \\\\\n",
    "\\text{Сам градиент можно рассчитать по формуле:} \\\\\n",
    "\\nabla f'_{\\beta_1} = \\frac{2}{N} \\sum_{i=1}^{N} x_1 (\\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n) \\\\\n",
    "\\nabla f = (f'_{\\beta_1},f'_{\\beta_2},f'_{\\beta_3},...,f'_{\\beta_n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Реализация ГРАДИЕНТНОГО СПУСКА из коробки\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "gradient_model_sklearn = SGDClassifier(fit_intercept=True, l1_ratio=0.15, max_iter = 10_000, alpha=0.0001)\n",
    "gradient_model_sklearn.fit(X.copy(), y.copy())\n",
    "preds_sklearn = gradient_model_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой RMSE в градиентном спуске:     72.96613624036382\n",
      "Sklearn RMSE в градиентном спуске: 81.71634963550935\n"
     ]
    }
   ],
   "source": [
    "#Ручная реализация ГРАДИЕНТНОГО СПУСКА\n",
    "class GradientDescent():\n",
    "    def __init__(self, step=0.0001, epsilon = 0.15):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.step = step\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X.copy()\n",
    "        self.X = np.c_[self.X, np.ones(self.X.shape[0])]\n",
    "        self.B = np.ones(X.shape[1] + 1)\n",
    "        self.y = y.copy()\n",
    "        prev_mse = self.MSE()\n",
    "        self._iteration()\n",
    "        next_mse = self.MSE()\n",
    "        while abs(next_mse - prev_mse) >= self.epsilon:\n",
    "            prev_mse = self.MSE()\n",
    "            self._iteration()\n",
    "            next_mse = self.MSE()\n",
    "        return self\n",
    "        \n",
    "    def _gradient(self):\n",
    "        return 2 * (self.X.dot(self.B) - self.y).dot(self.X) / self.X.shape[0]\n",
    "    \n",
    "    def MSE(self):\n",
    "        loss = (self.X.dot(self.B) - self.y) \n",
    "        return np.mean(loss**2)\n",
    "    \n",
    "    def _iteration(self):\n",
    "        self.B -= self.step * self._gradient()\n",
    "\n",
    "    def predict(self, X_train):\n",
    "        return np.c_[X_train, np.ones(X_train.shape[0])].dot(self.B)\n",
    "    \n",
    "gradient_model = GradientDescent()\n",
    "gradient_model.fit(X, y)\n",
    "preds_mine = gradient_model.predict(X_test)\n",
    "print('Мой RMSE в градиентном спуске:    ', np.sqrt(mse(y_test, preds_mine)))\n",
    "print('Sklearn RMSE в градиентном спуске:', np.sqrt(mse(y_test, preds_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Кросс-валидация} \\\\\n",
    "\\text{Кросс-валидация - процесс последовательного разделения данных на валидационную и обучающие выборки для подбора оптимальных параметров модели,} \\\\\n",
    "\\text{предотвращения переобучения и усреднения функционала качества. Алгоритм следующий: } \\\\\n",
    "\\text{данные делятся на N равных частей, затем N часть используется как валидационная, а остальная - как тренировочная.} \\\\\n",
    "\\text{Затем мы \"смещаемся\",  валидируем на N+1, но тренируем на другой и т.д., делам соответственно N раз.} \\\\\n",
    "\\text{После данный процедуры качество усредняется.} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{cross-validation}_{MSE} =\n",
    "\\begin{bmatrix} \n",
    "& \\textbf{Валидация} & \\text{Трейн} & \\text{Трейн} & \\text{Трейн} & \\text{Трейн} \\\\\n",
    "& \\text{Трейн} & \\textbf{Валидация} & \\text{Трейн} & \\text{Трейн} & \\text{Трейн} \\\\\n",
    "& \\text{Трейн} & \\text{Трейн} & \\textbf{Валидация} & \\text{Трейн} & \\text{Трейн} \\\\\n",
    "& \\text{Трейн} & \\text{Трейн} & \\text{Трейн} & \\textbf{Валидация} & \\text{Трейн} \\\\\n",
    "& \\text{Трейн} & \\text{Трейн} & \\text{Трейн} & \\text{Трейн} & \\textbf{Валидация} \\\\\n",
    "\\end{bmatrix} =>\n",
    "\\begin{bmatrix}\n",
    "MSE_1 \\\\\n",
    "MSE_2 \\\\\n",
    "... \\\\\n",
    "MSE_{N-1} \\\\\n",
    "MSE_N \\\\\n",
    "\\end{bmatrix} =\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}MSE_i$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation test RMSE  54.57185108354138\n",
      "Cross-validation train RMSE 53.444451847200455\n",
      "No CV RMSE                  53.85344583676592\n"
     ]
    }
   ],
   "source": [
    "#Реализация КРОСС-ВАЛИДАЦИИ из коробки\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "model = LinearRegression()\n",
    "\n",
    "cv_result = cross_validate(model, X, y, cv=splitter, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "print('Cross-validation test RMSE ', np.sqrt(-np.mean(cv_result['test_score'])))\n",
    "print('Cross-validation train RMSE', np.sqrt(-np.mean(cv_result['train_score'])))\n",
    "print('No CV RMSE                 ', no_cross_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation test RMSE  54.51587998454352\n",
      "Cross-validation train RMSE 53.44102090721621\n"
     ]
    }
   ],
   "source": [
    "#Ручная реализация КРОСС-ВАЛИДАЦИИ\n",
    "means_test = []\n",
    "means_train = []\n",
    "for train, test in splitter.split(X):\n",
    "   X_train, X_test = X[train], X[test]\n",
    "   y_train, y_test = y[train], y[test]\n",
    "\n",
    "   model = LinearRegression() \n",
    "   model.fit(X_train, y_train)\n",
    "   preds_test = model.predict(X_test)\n",
    "   preds_train = model.predict(X_train)\n",
    "\n",
    "   means_test.append(mse(y_test, preds_test) ** 0.5)\n",
    "   means_train.append(mse(y_train, preds_train) ** 0.5)\n",
    "print('Cross-validation test RMSE ', np.mean(means_test))\n",
    "print('Cross-validation train RMSE', np.mean(means_train))\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Регуляризация} \\\\\n",
    "\\text{Регуляризация - изменения признаков, приведение их к небольшим значениям в целях избежать переобучения на train выборке.} \\\\\n",
    "\\text{Выделяют два вида регуляризации: L1 (lasso) и L2 (ridge).} \\\\\n",
    "\\textbf{L1}\\text{ - в качестве регуляризатора берём сумму всех модулей весов модели.} \\\\\n",
    "R(\\beta) = \\sum_{i=1}^{n} {|\\beta_1^2|} = \\beta_1^2 + \\beta_2^2 + ... \\beta_n^2 \\\\ ||y - X_\\beta ||_2^2 + \\lambda||\\beta||_1 \\rightarrow min \\\\ \n",
    "\\textbf{L2} \\text{ - в качестве регуляризатора используется сумма квадратов весов модели.} \\\\\n",
    "R(\\beta) = \\sum_{i=1}^{n} {\\beta_i^2} = \\beta_1^2 + \\beta_2^2 + ... +\\beta_n^2 \\\\ ||y - X_\\beta ||_2^2 + \\lambda||\\beta||_2^2 \\rightarrow min \\\\\n",
    "\\textbf{Elastic Net}\\text{ - совмещённая L1 и L2 регуляризация:} \\\\\n",
    "||y - X_\\beta||^2 + \\lambda||\\beta||_2^2 + \\lambda||\\beta||_1 ъ\\\\\n",
    "\\text{Аналитическое решение для L2 регуляризации:} \\\\\n",
    "\\beta = (X^T \\cdot X + \\lambda I)^{-1} X^T y \\\\\n",
    "\\text{L1 регуляризация не имеет аналитического решения, ввиду абсолютного значения коэффициентов,} \\\\\n",
    "\\text{и, как следствие, отсутствия производной для } |x|\n",
    "\\text{ в точке 0} <=> |x|' = \\frac{x}{|x|}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 Ridge регуляризация из коробки\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=0)\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X_test)\n",
    "rmse_sklearn = mse(y_test, preds) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE нашей ridge модели:    53.37368921191776\n",
      "RMSE sklearn ridge модели:  53.37368921191778\n"
     ]
    }
   ],
   "source": [
    "#Ручная реализация L2 регуляризации\n",
    "class RidgeLR(LR):\n",
    "    def __init__(self, alpha=0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.rmse = 0\n",
    "    def fit(self, X, y):\n",
    "        X = X = np.c_[X, np.ones(X.shape[0])]\n",
    "        stage1 = np.dot(X.T, X)\n",
    "        stage2 = stage1 + self.alpha * np.ones(stage1.shape)\n",
    "        stage3 = np.linalg.inv(stage2)\n",
    "        stage4 = stage3.dot(X.T)\n",
    "        self.B = stage4.dot(y)\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]\n",
    "        return X_test.dot(self.B)\n",
    "    \n",
    "    \n",
    "ridge_model = RidgeLR()\n",
    "ridge_model.fit(X, y)\n",
    "preds = ridge_model.predict(X_test)\n",
    "rmse_mine = mse(y_test, preds) ** 0.5\n",
    "print('RMSE нашей ridge модели:   ', rmse_mine)\n",
    "print('RMSE sklearn ridge модели: ', rmse_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ручная реализация L1 регуляризации\n",
    "class LassoGD(GradientDescent):\n",
    "    def __init__(self, alpha= 0.1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    def _gradient(self):\n",
    "        sign = lambda x: 1 if x >= 0 else -1\n",
    "        sign_m = np.vectorize(sign)(self.B)\n",
    "        return 2 * (self.X.dot(self.B) - self.y).dot(self.X) / self.X.shape[0] + self.alpha * sign_m\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn RMSE : 55.642241522866364\n",
      "моё     RMSE : 82.19780068628188\n"
     ]
    }
   ],
   "source": [
    "#L1 Lasso регуляризация из коробки\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(alpha=0.1, penalty='l1', max_iter=1000, eta0 = 0.15)\n",
    "sgd_reg.fit(X, y)\n",
    "preds1 = sgd_reg.predict(X_test)\n",
    "rmse1 = mse(y_test, preds1) ** 0.5\n",
    "\n",
    "gd = LassoGD(alpha = 0.1, epsilon = 0.15)\n",
    "gd.fit(X, y)\n",
    "preds2 = gd.predict(X_test)\n",
    "rmse2 = mse(y_test, preds2) ** 0.5\n",
    "print('sklearn RMSE :', rmse1)\n",
    "print('моё     RMSE :', rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ручная реализация Elastic Net регуляризации в градиентном спуске\n",
    "class ElasticNetGD(GradientDescent):\n",
    "    def __init__(self, alpha = 1, l1_ratio=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "    def _gradient(self):\n",
    "        sign = lambda x: 1 if x >= 0 else -1\n",
    "        sign_m = np.vectorize(sign)(self.B)\n",
    "        gradient = (self.X.dot(self.B) - self.y).dot(self.X) / self.X.shape[0]\n",
    "        gradient -= self.alpha * ((1 - self.l1_ratio) * self.B + self.l1_ratio * sign_m)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn RMSE : 71.49971078183998\n",
      "моё     RMSE : 75.64664472847011\n"
     ]
    }
   ],
   "source": [
    "#Elastic Net регуляризаця из коробки\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha = 0.1)\n",
    "sk_preds = enet.fit(X, y).predict(X_test)\n",
    "d = enet.get_params()\n",
    "\n",
    "model = ElasticNetGD(epsilon = 0.01, alpha = d['alpha'], l1_ratio=d['l1_ratio'])\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print('sklearn RMSE :',mse(y_test, sk_preds)**0.5)\n",
    "print('моё     RMSE :',mse(y_test, preds)**0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
